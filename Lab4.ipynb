{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab Assignment Four: Multi-Layer Perceptron"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.Load, Split, and Balance\n",
    "\n",
    "- [.5 points] Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the variables until asked to do so.  Remove any observations that having missing data. Encode any string data as integers for now. \n",
    "- [.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing.\n",
    "- [.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numpy version is: 1.19.2\npandas version is: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "#import the lib needed \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "print(\"numpy version is:\",np.__version__)\n",
    "print(\"pandas version is:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            TractId      TotalPop           Men         Women      Hispanic  \\\n",
       "count  7.400100e+04  74001.000000  74001.000000  74001.000000  73305.000000   \n",
       "mean   2.839113e+10   4384.716017   2157.710707   2227.005311     17.265444   \n",
       "std    1.647593e+10   2228.936729   1120.560504   1146.240218     23.073811   \n",
       "min    1.001020e+09      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    1.303901e+10   2903.000000   1416.000000   1465.000000      2.600000   \n",
       "50%    2.804700e+10   4105.000000   2007.000000   2082.000000      7.400000   \n",
       "75%    4.200341e+10   5506.000000   2707.000000   2803.000000     21.100000   \n",
       "max    7.215375e+10  65528.000000  32266.000000  33262.000000    100.000000   \n",
       "\n",
       "              White        Black        Native         Asian       Pacific  \\\n",
       "count  73305.000000  73305.00000  73305.000000  73305.000000  73305.000000   \n",
       "mean      61.309043     13.28910      0.734047      4.753691      0.147341   \n",
       "std       30.634461     21.60118      4.554247      8.999888      1.029250   \n",
       "min        0.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%       38.000000      0.80000      0.000000      0.200000      0.000000   \n",
       "50%       70.400000      3.80000      0.000000      1.500000      0.000000   \n",
       "75%       87.700000     14.60000      0.400000      5.000000      0.000000   \n",
       "max      100.000000    100.00000    100.000000    100.000000     71.900000   \n",
       "\n",
       "       ...          Walk   OtherTransp    WorkAtHome   MeanCommute  \\\n",
       "count  ...  73200.000000  73200.000000  73200.000000  73055.000000   \n",
       "mean   ...      3.042825      1.894605      4.661466     26.056594   \n",
       "std    ...      5.805753      2.549374      4.014940      7.124524   \n",
       "min    ...      0.000000      0.000000      0.000000      1.000000   \n",
       "25%    ...      0.400000      0.400000      2.000000     21.100000   \n",
       "50%    ...      1.400000      1.200000      3.800000     25.400000   \n",
       "75%    ...      3.300000      2.500000      6.300000     30.300000   \n",
       "max    ...    100.000000    100.000000    100.000000     73.900000   \n",
       "\n",
       "           Employed   PrivateWork    PublicWork  SelfEmployed    FamilyWork  \\\n",
       "count  74001.000000  73190.000000  73190.000000  73190.000000  73190.000000   \n",
       "mean    2049.152052     79.494222     14.163342      6.171484      0.171164   \n",
       "std     1138.865457      8.126383      7.328680      3.932364      0.456580   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     1276.000000     75.200000      9.300000      3.500000      0.000000   \n",
       "50%     1895.000000     80.600000     13.000000      5.500000      0.000000   \n",
       "75%     2635.000000     85.000000     17.600000      8.000000      0.000000   \n",
       "max    28945.000000    100.000000    100.000000    100.000000     22.300000   \n",
       "\n",
       "       Unemployment  \n",
       "count  73191.000000  \n",
       "mean       7.246738  \n",
       "std        5.227624  \n",
       "min        0.000000  \n",
       "25%        3.900000  \n",
       "50%        6.000000  \n",
       "75%        9.000000  \n",
       "max      100.000000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TractId</th>\n      <th>TotalPop</th>\n      <th>Men</th>\n      <th>Women</th>\n      <th>Hispanic</th>\n      <th>White</th>\n      <th>Black</th>\n      <th>Native</th>\n      <th>Asian</th>\n      <th>Pacific</th>\n      <th>...</th>\n      <th>Walk</th>\n      <th>OtherTransp</th>\n      <th>WorkAtHome</th>\n      <th>MeanCommute</th>\n      <th>Employed</th>\n      <th>PrivateWork</th>\n      <th>PublicWork</th>\n      <th>SelfEmployed</th>\n      <th>FamilyWork</th>\n      <th>Unemployment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7.400100e+04</td>\n      <td>74001.000000</td>\n      <td>74001.000000</td>\n      <td>74001.000000</td>\n      <td>73305.000000</td>\n      <td>73305.000000</td>\n      <td>73305.00000</td>\n      <td>73305.000000</td>\n      <td>73305.000000</td>\n      <td>73305.000000</td>\n      <td>...</td>\n      <td>73200.000000</td>\n      <td>73200.000000</td>\n      <td>73200.000000</td>\n      <td>73055.000000</td>\n      <td>74001.000000</td>\n      <td>73190.000000</td>\n      <td>73190.000000</td>\n      <td>73190.000000</td>\n      <td>73190.000000</td>\n      <td>73191.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.839113e+10</td>\n      <td>4384.716017</td>\n      <td>2157.710707</td>\n      <td>2227.005311</td>\n      <td>17.265444</td>\n      <td>61.309043</td>\n      <td>13.28910</td>\n      <td>0.734047</td>\n      <td>4.753691</td>\n      <td>0.147341</td>\n      <td>...</td>\n      <td>3.042825</td>\n      <td>1.894605</td>\n      <td>4.661466</td>\n      <td>26.056594</td>\n      <td>2049.152052</td>\n      <td>79.494222</td>\n      <td>14.163342</td>\n      <td>6.171484</td>\n      <td>0.171164</td>\n      <td>7.246738</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.647593e+10</td>\n      <td>2228.936729</td>\n      <td>1120.560504</td>\n      <td>1146.240218</td>\n      <td>23.073811</td>\n      <td>30.634461</td>\n      <td>21.60118</td>\n      <td>4.554247</td>\n      <td>8.999888</td>\n      <td>1.029250</td>\n      <td>...</td>\n      <td>5.805753</td>\n      <td>2.549374</td>\n      <td>4.014940</td>\n      <td>7.124524</td>\n      <td>1138.865457</td>\n      <td>8.126383</td>\n      <td>7.328680</td>\n      <td>3.932364</td>\n      <td>0.456580</td>\n      <td>5.227624</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.001020e+09</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.303901e+10</td>\n      <td>2903.000000</td>\n      <td>1416.000000</td>\n      <td>1465.000000</td>\n      <td>2.600000</td>\n      <td>38.000000</td>\n      <td>0.80000</td>\n      <td>0.000000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>2.000000</td>\n      <td>21.100000</td>\n      <td>1276.000000</td>\n      <td>75.200000</td>\n      <td>9.300000</td>\n      <td>3.500000</td>\n      <td>0.000000</td>\n      <td>3.900000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.804700e+10</td>\n      <td>4105.000000</td>\n      <td>2007.000000</td>\n      <td>2082.000000</td>\n      <td>7.400000</td>\n      <td>70.400000</td>\n      <td>3.80000</td>\n      <td>0.000000</td>\n      <td>1.500000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.400000</td>\n      <td>1.200000</td>\n      <td>3.800000</td>\n      <td>25.400000</td>\n      <td>1895.000000</td>\n      <td>80.600000</td>\n      <td>13.000000</td>\n      <td>5.500000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.200341e+10</td>\n      <td>5506.000000</td>\n      <td>2707.000000</td>\n      <td>2803.000000</td>\n      <td>21.100000</td>\n      <td>87.700000</td>\n      <td>14.60000</td>\n      <td>0.400000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>3.300000</td>\n      <td>2.500000</td>\n      <td>6.300000</td>\n      <td>30.300000</td>\n      <td>2635.000000</td>\n      <td>85.000000</td>\n      <td>17.600000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.215375e+10</td>\n      <td>65528.000000</td>\n      <td>32266.000000</td>\n      <td>33262.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.00000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>71.900000</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>73.900000</td>\n      <td>28945.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>22.300000</td>\n      <td>100.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('./data/acs2017_census_tract_data.csv')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "df.State = df.State.astype('category')\n",
    "df.County = df.County.astype('category')\n",
    "df.State = df.State.cat.codes\n",
    "df.County = df.County.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TractId               int64\n",
       "State                  int8\n",
       "County                int16\n",
       "TotalPop              int64\n",
       "Men                   int64\n",
       "Women                 int64\n",
       "Hispanic            float64\n",
       "White               float64\n",
       "Black               float64\n",
       "Native              float64\n",
       "Asian               float64\n",
       "Pacific             float64\n",
       "VotingAgeCitizen      int64\n",
       "Income              float64\n",
       "IncomeErr           float64\n",
       "IncomePerCap        float64\n",
       "IncomePerCapErr     float64\n",
       "Poverty             float64\n",
       "ChildPoverty        float64\n",
       "Professional        float64\n",
       "Service             float64\n",
       "Office              float64\n",
       "Construction        float64\n",
       "Production          float64\n",
       "Drive               float64\n",
       "Carpool             float64\n",
       "Transit             float64\n",
       "Walk                float64\n",
       "OtherTransp         float64\n",
       "WorkAtHome          float64\n",
       "MeanCommute         float64\n",
       "Employed              int64\n",
       "PrivateWork         float64\n",
       "PublicWork          float64\n",
       "SelfEmployed        float64\n",
       "FamilyWork          float64\n",
       "Unemployment        float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "California              8057\n",
       "Texas                   5265\n",
       "New York                4918\n",
       "Florida                 4245\n",
       "Pennsylvania            3218\n",
       "Illinois                3123\n",
       "Ohio                    2952\n",
       "Michigan                2813\n",
       "North Carolina          2195\n",
       "New Jersey              2010\n",
       "Georgia                 1969\n",
       "Virginia                1907\n",
       "Arizona                 1526\n",
       "Indiana                 1511\n",
       "Tennessee               1497\n",
       "Massachusetts           1478\n",
       "Washington              1458\n",
       "Wisconsin               1409\n",
       "Maryland                1406\n",
       "Missouri                1393\n",
       "Minnesota               1338\n",
       "Colorado                1249\n",
       "Alabama                 1181\n",
       "Louisiana               1148\n",
       "Kentucky                1115\n",
       "South Carolina          1103\n",
       "Oklahoma                1046\n",
       "Puerto Rico              945\n",
       "Oregon                   834\n",
       "Connecticut              833\n",
       "Iowa                     825\n",
       "Kansas                   770\n",
       "Nevada                   687\n",
       "Arkansas                 686\n",
       "Mississippi              664\n",
       "Utah                     588\n",
       "Nebraska                 532\n",
       "New Mexico               499\n",
       "West Virginia            484\n",
       "Maine                    358\n",
       "Hawaii                   351\n",
       "Idaho                    298\n",
       "New Hampshire            295\n",
       "Montana                  271\n",
       "Rhode Island             244\n",
       "South Dakota             222\n",
       "Delaware                 218\n",
       "North Dakota             205\n",
       "Vermont                  184\n",
       "District of Columbia     179\n",
       "Alaska                   167\n",
       "Wyoming                  132\n",
       "Name: State, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.State.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Autauga County', 'Baldwin County', 'Barbour County', 'Bibb County', 'Blount County', ..., 'Vega Baja Municipio', 'Vieques Municipio', 'Villalba Municipio', 'Yabucoa Municipio', 'Yauco Municipio']\n",
       "Length: 1955\n",
       "Categories (1955, object): ['Autauga County', 'Baldwin County', 'Barbour County', 'Bibb County', ..., 'Vieques Municipio', 'Villalba Municipio', 'Yabucoa Municipio', 'Yauco Municipio']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.County.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TractId                0\nState                  0\nCounty                 0\nTotalPop               0\nMen                    0\nWomen                  0\nHispanic             696\nWhite                696\nBlack                696\nNative               696\nAsian                696\nPacific              696\nVotingAgeCitizen       0\nIncome              1116\nIncomeErr           1116\nIncomePerCap         745\nIncomePerCapErr      745\nPoverty              842\nChildPoverty        1110\nProfessional         811\nService              811\nOffice               811\nConstruction         811\nProduction           811\nDrive                801\nCarpool              801\nTransit              801\nWalk                 801\nOtherTransp          801\nWorkAtHome           801\nMeanCommute          946\nEmployed               0\nPrivateWork          811\nPublicWork           811\nSelfEmployed         811\nFamilyWork           811\nUnemployment         810\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            TractId      TotalPop           Men         Women      Hispanic  \\\n",
       "count  7.271800e+04  72718.000000  72718.000000  72718.000000  72718.000000   \n",
       "mean   2.837292e+10   4443.485121   2184.362647   2259.122473     17.282951   \n",
       "std    1.644198e+10   2190.183318   1099.954423   1124.604806     23.084428   \n",
       "min    1.001020e+09     58.000000     26.000000     27.000000      0.000000   \n",
       "25%    1.304703e+10   2958.000000   1440.000000   1494.000000      2.600000   \n",
       "50%    2.804700e+10   4137.000000   2024.000000   2102.000000      7.400000   \n",
       "75%    4.200341e+10   5532.750000   2719.000000   2817.000000     21.100000   \n",
       "max    7.215375e+10  65528.000000  32266.000000  33262.000000    100.000000   \n",
       "\n",
       "              White         Black        Native         Asian       Pacific  \\\n",
       "count  72718.000000  72718.000000  72718.000000  72718.000000  72718.000000   \n",
       "mean      61.337143     13.254417      0.727776      4.752459      0.146082   \n",
       "std       30.628031     21.581269      4.505791      8.995573      1.015198   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       38.100000      0.800000      0.000000      0.200000      0.000000   \n",
       "50%       70.500000      3.800000      0.000000      1.500000      0.000000   \n",
       "75%       87.700000     14.500000      0.400000      5.000000      0.000000   \n",
       "max      100.000000    100.000000     99.400000     91.400000     71.900000   \n",
       "\n",
       "       ...          Walk   OtherTransp    WorkAtHome   MeanCommute  \\\n",
       "count  ...  72718.000000  72718.000000  72718.000000  72718.000000   \n",
       "mean   ...      2.925778      1.886076      4.612646     26.080334   \n",
       "std    ...      5.260623      2.485812      3.770733      7.095680   \n",
       "min    ...      0.000000      0.000000      0.000000      4.200000   \n",
       "25%    ...      0.400000      0.400000      2.000000     21.100000   \n",
       "50%    ...      1.400000      1.200000      3.800000     25.400000   \n",
       "75%    ...      3.300000      2.500000      6.300000     30.300000   \n",
       "max    ...     77.700000     53.400000     82.800000     73.900000   \n",
       "\n",
       "           Employed   PrivateWork    PublicWork  SelfEmployed    FamilyWork  \\\n",
       "count  72718.000000  72718.000000  72718.000000  72718.000000  72718.000000   \n",
       "mean    2081.309139     79.511827     14.149495      6.167661      0.171231   \n",
       "std     1120.109805      7.957350      7.164790      3.798703      0.451630   \n",
       "min       20.000000     17.500000      0.000000      0.000000      0.000000   \n",
       "25%     1306.000000     75.300000      9.300000      3.500000      0.000000   \n",
       "50%     1915.000000     80.600000     13.000000      5.500000      0.000000   \n",
       "75%     2651.000000     85.000000     17.600000      8.000000      0.000000   \n",
       "max    28945.000000    100.000000     80.700000     47.400000     22.300000   \n",
       "\n",
       "       Unemployment  \n",
       "count  72718.000000  \n",
       "mean       7.224917  \n",
       "std        5.099419  \n",
       "min        0.000000  \n",
       "25%        3.900000  \n",
       "50%        6.000000  \n",
       "75%        9.000000  \n",
       "max       62.800000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TractId</th>\n      <th>TotalPop</th>\n      <th>Men</th>\n      <th>Women</th>\n      <th>Hispanic</th>\n      <th>White</th>\n      <th>Black</th>\n      <th>Native</th>\n      <th>Asian</th>\n      <th>Pacific</th>\n      <th>...</th>\n      <th>Walk</th>\n      <th>OtherTransp</th>\n      <th>WorkAtHome</th>\n      <th>MeanCommute</th>\n      <th>Employed</th>\n      <th>PrivateWork</th>\n      <th>PublicWork</th>\n      <th>SelfEmployed</th>\n      <th>FamilyWork</th>\n      <th>Unemployment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7.271800e+04</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>...</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.837292e+10</td>\n      <td>4443.485121</td>\n      <td>2184.362647</td>\n      <td>2259.122473</td>\n      <td>17.282951</td>\n      <td>61.337143</td>\n      <td>13.254417</td>\n      <td>0.727776</td>\n      <td>4.752459</td>\n      <td>0.146082</td>\n      <td>...</td>\n      <td>2.925778</td>\n      <td>1.886076</td>\n      <td>4.612646</td>\n      <td>26.080334</td>\n      <td>2081.309139</td>\n      <td>79.511827</td>\n      <td>14.149495</td>\n      <td>6.167661</td>\n      <td>0.171231</td>\n      <td>7.224917</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.644198e+10</td>\n      <td>2190.183318</td>\n      <td>1099.954423</td>\n      <td>1124.604806</td>\n      <td>23.084428</td>\n      <td>30.628031</td>\n      <td>21.581269</td>\n      <td>4.505791</td>\n      <td>8.995573</td>\n      <td>1.015198</td>\n      <td>...</td>\n      <td>5.260623</td>\n      <td>2.485812</td>\n      <td>3.770733</td>\n      <td>7.095680</td>\n      <td>1120.109805</td>\n      <td>7.957350</td>\n      <td>7.164790</td>\n      <td>3.798703</td>\n      <td>0.451630</td>\n      <td>5.099419</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.001020e+09</td>\n      <td>58.000000</td>\n      <td>26.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.200000</td>\n      <td>20.000000</td>\n      <td>17.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.304703e+10</td>\n      <td>2958.000000</td>\n      <td>1440.000000</td>\n      <td>1494.000000</td>\n      <td>2.600000</td>\n      <td>38.100000</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>2.000000</td>\n      <td>21.100000</td>\n      <td>1306.000000</td>\n      <td>75.300000</td>\n      <td>9.300000</td>\n      <td>3.500000</td>\n      <td>0.000000</td>\n      <td>3.900000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.804700e+10</td>\n      <td>4137.000000</td>\n      <td>2024.000000</td>\n      <td>2102.000000</td>\n      <td>7.400000</td>\n      <td>70.500000</td>\n      <td>3.800000</td>\n      <td>0.000000</td>\n      <td>1.500000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.400000</td>\n      <td>1.200000</td>\n      <td>3.800000</td>\n      <td>25.400000</td>\n      <td>1915.000000</td>\n      <td>80.600000</td>\n      <td>13.000000</td>\n      <td>5.500000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.200341e+10</td>\n      <td>5532.750000</td>\n      <td>2719.000000</td>\n      <td>2817.000000</td>\n      <td>21.100000</td>\n      <td>87.700000</td>\n      <td>14.500000</td>\n      <td>0.400000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>3.300000</td>\n      <td>2.500000</td>\n      <td>6.300000</td>\n      <td>30.300000</td>\n      <td>2651.000000</td>\n      <td>85.000000</td>\n      <td>17.600000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.215375e+10</td>\n      <td>65528.000000</td>\n      <td>32266.000000</td>\n      <td>33262.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>99.400000</td>\n      <td>91.400000</td>\n      <td>71.900000</td>\n      <td>...</td>\n      <td>77.700000</td>\n      <td>53.400000</td>\n      <td>82.800000</td>\n      <td>73.900000</td>\n      <td>28945.000000</td>\n      <td>100.000000</td>\n      <td>80.700000</td>\n      <td>47.400000</td>\n      <td>22.300000</td>\n      <td>62.800000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# drop the rows contained NA data. \n",
    "df = df.dropna()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TractId             0\nState               0\nCounty              0\nTotalPop            0\nMen                 0\nWomen               0\nHispanic            0\nWhite               0\nBlack               0\nNative              0\nAsian               0\nPacific             0\nVotingAgeCitizen    0\nIncome              0\nIncomeErr           0\nIncomePerCap        0\nIncomePerCapErr     0\nPoverty             0\nChildPoverty        0\nProfessional        0\nService             0\nOffice              0\nConstruction        0\nProduction          0\nDrive               0\nCarpool             0\nTransit             0\nWalk                0\nOtherTransp         0\nWorkAtHome          0\nMeanCommute         0\nEmployed            0\nPrivateWork         0\nPublicWork          0\nSelfEmployed        0\nFamilyWork          0\nUnemployment        0\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TractId               int64\n",
       "State                object\n",
       "County               object\n",
       "TotalPop              int64\n",
       "Men                   int64\n",
       "Women                 int64\n",
       "Hispanic            float64\n",
       "White               float64\n",
       "Black               float64\n",
       "Native              float64\n",
       "Asian               float64\n",
       "Pacific             float64\n",
       "VotingAgeCitizen      int64\n",
       "Income              float64\n",
       "IncomeErr           float64\n",
       "IncomePerCap        float64\n",
       "IncomePerCapErr     float64\n",
       "Poverty             float64\n",
       "ChildPoverty        float64\n",
       "Professional        float64\n",
       "Service             float64\n",
       "Office              float64\n",
       "Construction        float64\n",
       "Production          float64\n",
       "Drive               float64\n",
       "Carpool             float64\n",
       "Transit             float64\n",
       "Walk                float64\n",
       "OtherTransp         float64\n",
       "WorkAtHome          float64\n",
       "MeanCommute         float64\n",
       "Employed              int64\n",
       "PrivateWork         float64\n",
       "PublicWork          float64\n",
       "SelfEmployed        float64\n",
       "FamilyWork          float64\n",
       "Unemployment        float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": []
  },
  {
   "source": [
    "## 2.Pre-processing\n",
    "\n",
    "There are a number of version of the two layer perceptron covered in class. When using the example two layer network from class be sure that you use: (1) vectorized computation, (2) mini-batching, and (3) proper Glorot initialization, at a minimum.  \n",
    "\n",
    "- [.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "- [.5 points] Now normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n",
    "- [.5 points] Now normalize the continuous numeric feature data AND one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "- [1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "    - Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 3.Modeling\n",
    "\n",
    "- [1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch. For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm. Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n",
    "- [1 points] Repeat the previous step, adding support for a fourth layer.\n",
    "- [1 points] Repeat the previous step, adding support for a fifth layer. \n",
    "- [2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network. Compare the performance of this model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 4.Exceptional Work\n",
    "\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}