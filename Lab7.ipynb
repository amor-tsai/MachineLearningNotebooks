{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab Assignment Seven: Recurrent Network Architectures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Select a dataset that is text. That is, the dataset should be text data (or a time series sequence). In terms of generalization performance, it is helpful to have a large dataset of similar sized text documents. It is fine to perform binary classification or multi-class classification. The classification can be \"many-to-one\" or \"many-to-many\" sequence classification, whichever you feel more comfortable with. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "It's a dataset I got from [kaggle](https://www.kaggle.com/kazanova/sentiment140). Sentiment140 dataset with 1.6 million tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preparation (3 points total)\n",
    "- [1 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.  \n",
    "- [1 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "- [1 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your train/test splitting method is a realistic mirroring of how an algorithm would be used in practice. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorflow version: 2.3.0\npandas version: 1.2.4\nkeras version: 2.4.0\nnumpy version: 1.18.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adagrad,Adam\n",
    "\n",
    "print('tensorflow version:',tf.__version__)\n",
    "print('pandas version:',pd.__version__)\n",
    "print('keras version:',keras.__version__)\n",
    "print('numpy version:',np.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "source": [
    "### It's a large dataset contained 1.6 million rows, but I use only 10 thousand rows of them for computational efficiency."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 800000 to 799999\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   target  10000 non-null  int64 \n 1   text    10000 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 234.4+ KB\nCPU times: user 638 ms, sys: 7.83 ms, total: 646 ms\nWall time: 645 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chunks = pd.read_csv('./data/sentiment140.csv',names=['target','ids','date','flag','user','text'],usecols=['target','text'],iterator=True,chunksize=5000)\n",
    "get_pos = False\n",
    "get_neg = False\n",
    "for chunk in chunks:\n",
    "    if ~get_pos and (chunk['target']==4).all():\n",
    "        chunka = chunk\n",
    "        get_pos = True\n",
    "    if ~get_neg and (chunk['target']==0).all():\n",
    "        chunkb = chunk\n",
    "        get_neg = True\n",
    "    if  get_pos and get_neg:\n",
    "        break\n",
    "\n",
    "df = pd.concat([chunka,chunkb])\n",
    "df['target'].replace(4,1,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    5000\n",
       "1    5000\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Modeling (6 points total)\n",
    "- [3 points] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Be sure to use an embedding layer (pre-trained, from scratch, OR both). Adjust hyper-parameters of the networks as needed to improve generalization performance (train a total of at least four models). Discuss the performance of each network and compare them.\n",
    "- [1 points] Using the best RNN parameters and architecture, add a second recurrent chain to your RNN. The input to the second chain should be the output sequence of the first chain. Visualize the performance of training and validation sets versus the training iterations. \n",
    "- [2 points] Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Exceptional Work (1 points total)\n",
    "- You have free reign to provide additional analyses.\n",
    "- One idea (required for 7000 level students to do one of these options):\n",
    "    - Option 1: Use dimensionality reduction (choose an appropriate method from this list: t-SNE, SVD, PCA, or UMAP) to visualize the word embeddings of a subset of words in your vocabulary that you expect to have an analogy that can be captured by the embedding. Try to interpret if an analogy exists, show the vectors that support/refute the analogy, and interpret your findings. \n",
    "    - Options 2: Use the ConceptNet Numberbatch embedding and compare to GloVe. Which method is better for your specific application? \n",
    "- Another Idea (NOT required): Try to create a RNN for generating novel text. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}